# Kubernetes CronJob for Automated Backups - Option D Implementation
# Scheduled backup operations in production environment

apiVersion: batch/v1
kind: CronJob
metadata:
  name: backup-full
  namespace: production
  labels:
    app: ads-pro-platform
    component: backup
    environment: production
spec:
  # Weekly full backup every Sunday at 2:00 AM UTC
  schedule: "0 2 * * 0"
  timeZone: "UTC"
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  startingDeadlineSeconds: 300
  jobTemplate:
    spec:
      backoffLimit: 2
      activeDeadlineSeconds: 3600  # 1 hour timeout
      template:
        metadata:
          labels:
            app: ads-pro-platform
            component: backup
            backup-type: full
        spec:
          restartPolicy: OnFailure
          serviceAccountName: backup-service-account
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
            runAsGroup: 1001
            fsGroup: 1001
          containers:
          - name: backup-full
            image: postgres:15-alpine
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - |
              set -e
              
              echo "Starting full backup process..."
              
              # Install required tools
              apk add --no-cache curl aws-cli
              
              # Set backup filename with timestamp
              TIMESTAMP=$(date '+%Y%m%d_%H%M%S')
              BACKUP_FILE="/backups/full_backup_${TIMESTAMP}.dump"
              
              # Perform database backup
              echo "Creating database backup..."
              pg_dump -h $POSTGRES_HOST -p $POSTGRES_PORT -U $POSTGRES_USER -d $POSTGRES_DB \
                      --verbose --clean --if-exists --create \
                      --format=custom --compress=9 \
                      --file="$BACKUP_FILE"
              
              # Verify backup
              echo "Verifying backup..."
              if pg_restore --list "$BACKUP_FILE" > /dev/null; then
                echo "Backup verification successful"
              else
                echo "Backup verification failed"
                exit 1
              fi
              
              # Generate checksum
              sha256sum "$BACKUP_FILE" > "${BACKUP_FILE}.sha256"
              
              # Upload to S3 if configured
              if [ -n "${AWS_S3_BACKUP_BUCKET:-}" ]; then
                echo "Uploading backup to S3..."
                aws s3 cp "$BACKUP_FILE" "s3://${AWS_S3_BACKUP_BUCKET}/full/" \
                    --storage-class STANDARD_IA
                aws s3 cp "${BACKUP_FILE}.sha256" "s3://${AWS_S3_BACKUP_BUCKET}/full/"
                echo "Backup uploaded successfully"
              fi
              
              # Send webhook notification
              if [ -n "${BACKUP_WEBHOOK_URL:-}" ]; then
                curl -X POST "${BACKUP_WEBHOOK_URL}" \
                     -H "Content-Type: application/json" \
                     -d "{\"type\":\"full_backup\",\"status\":\"success\",\"timestamp\":\"$(date -u +%Y-%m-%dT%H:%M:%SZ)\",\"file\":\"$(basename "$BACKUP_FILE")\"}"
              fi
              
              echo "Full backup completed successfully"
            env:
            - name: POSTGRES_HOST
              valueFrom:
                secretRef:
                  name: database-credentials
                  key: host
            - name: POSTGRES_PORT
              valueFrom:
                secretRef:
                  name: database-credentials
                  key: port
            - name: POSTGRES_DB
              valueFrom:
                secretRef:
                  name: database-credentials
                  key: database
            - name: POSTGRES_USER
              valueFrom:
                secretRef:
                  name: database-credentials
                  key: username
            - name: PGPASSWORD
              valueFrom:
                secretRef:
                  name: database-credentials
                  key: password
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretRef:
                  name: backup-credentials
                  key: aws-access-key-id
                  optional: true
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretRef:
                  name: backup-credentials
                  key: aws-secret-access-key
                  optional: true
            - name: AWS_S3_BACKUP_BUCKET
              valueFrom:
                configMapRef:
                  name: backup-config
                  key: s3-bucket
                  optional: true
            - name: BACKUP_WEBHOOK_URL
              valueFrom:
                secretRef:
                  name: backup-credentials
                  key: webhook-url
                  optional: true
            resources:
              requests:
                memory: "256Mi"
                cpu: "250m"
              limits:
                memory: "512Mi"
                cpu: "500m"
            volumeMounts:
            - name: backup-storage
              mountPath: /backups
            securityContext:
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: false
              runAsNonRoot: true
              runAsUser: 1001
              capabilities:
                drop:
                - ALL
          volumes:
          - name: backup-storage
            persistentVolumeClaim:
              claimName: backup-storage-pvc

---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: backup-incremental
  namespace: production
  labels:
    app: ads-pro-platform
    component: backup
    environment: production
spec:
  # Incremental backup every 6 hours
  schedule: "0 */6 * * *"
  timeZone: "UTC"
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 5
  failedJobsHistoryLimit: 3
  startingDeadlineSeconds: 300
  jobTemplate:
    spec:
      backoffLimit: 2
      activeDeadlineSeconds: 1800  # 30 minutes timeout
      template:
        metadata:
          labels:
            app: ads-pro-platform
            component: backup
            backup-type: incremental
        spec:
          restartPolicy: OnFailure
          serviceAccountName: backup-service-account
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
            runAsGroup: 1001
            fsGroup: 1001
          containers:
          - name: backup-incremental
            image: postgres:15-alpine
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - |
              set -e
              
              echo "Starting incremental backup process..."
              
              # Install required tools
              apk add --no-cache curl aws-cli
              
              # Set backup filename with timestamp
              TIMESTAMP=$(date '+%Y%m%d_%H%M%S')
              BACKUP_FILE="/backups/incremental_backup_${TIMESTAMP}.dump"
              
              # Perform incremental database backup (data only)
              echo "Creating incremental database backup..."
              pg_dump -h $POSTGRES_HOST -p $POSTGRES_PORT -U $POSTGRES_USER -d $POSTGRES_DB \
                      --verbose --data-only \
                      --format=custom --compress=9 \
                      --file="$BACKUP_FILE"
              
              # Verify backup
              echo "Verifying backup..."
              if pg_restore --list "$BACKUP_FILE" > /dev/null; then
                echo "Backup verification successful"
              else
                echo "Backup verification failed"
                exit 1
              fi
              
              # Generate checksum
              sha256sum "$BACKUP_FILE" > "${BACKUP_FILE}.sha256"
              
              # Upload to S3 if configured
              if [ -n "${AWS_S3_BACKUP_BUCKET:-}" ]; then
                echo "Uploading backup to S3..."
                aws s3 cp "$BACKUP_FILE" "s3://${AWS_S3_BACKUP_BUCKET}/incremental/" \
                    --storage-class STANDARD_IA
                aws s3 cp "${BACKUP_FILE}.sha256" "s3://${AWS_S3_BACKUP_BUCKET}/incremental/"
                echo "Backup uploaded successfully"
              fi
              
              # Send webhook notification
              if [ -n "${BACKUP_WEBHOOK_URL:-}" ]; then
                curl -X POST "${BACKUP_WEBHOOK_URL}" \
                     -H "Content-Type: application/json" \
                     -d "{\"type\":\"incremental_backup\",\"status\":\"success\",\"timestamp\":\"$(date -u +%Y-%m-%dT%H:%M:%SZ)\",\"file\":\"$(basename "$BACKUP_FILE")\"}"
              fi
              
              echo "Incremental backup completed successfully"
            env:
            - name: POSTGRES_HOST
              valueFrom:
                secretRef:
                  name: database-credentials
                  key: host
            - name: POSTGRES_PORT
              valueFrom:
                secretRef:
                  name: database-credentials
                  key: port
            - name: POSTGRES_DB
              valueFrom:
                secretRef:
                  name: database-credentials
                  key: database
            - name: POSTGRES_USER
              valueFrom:
                secretRef:
                  name: database-credentials
                  key: username
            - name: PGPASSWORD
              valueFrom:
                secretRef:
                  name: database-credentials
                  key: password
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretRef:
                  name: backup-credentials
                  key: aws-access-key-id
                  optional: true
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretRef:
                  name: backup-credentials
                  key: aws-secret-access-key
                  optional: true
            - name: AWS_S3_BACKUP_BUCKET
              valueFrom:
                configMapRef:
                  name: backup-config
                  key: s3-bucket
                  optional: true
            - name: BACKUP_WEBHOOK_URL
              valueFrom:
                secretRef:
                  name: backup-credentials
                  key: webhook-url
                  optional: true
            resources:
              requests:
                memory: "128Mi"
                cpu: "100m"
              limits:
                memory: "256Mi"
                cpu: "250m"
            volumeMounts:
            - name: backup-storage
              mountPath: /backups
            securityContext:
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: false
              runAsNonRoot: true
              runAsUser: 1001
              capabilities:
                drop:
                - ALL
          volumes:
          - name: backup-storage
            persistentVolumeClaim:
              claimName: backup-storage-pvc

---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: backup-cleanup
  namespace: production
  labels:
    app: ads-pro-platform
    component: backup
    environment: production
spec:
  # Daily cleanup at 4:00 AM UTC
  schedule: "0 4 * * *"
  timeZone: "UTC"
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
  startingDeadlineSeconds: 300
  jobTemplate:
    spec:
      backoffLimit: 1
      activeDeadlineSeconds: 600  # 10 minutes timeout
      template:
        metadata:
          labels:
            app: ads-pro-platform
            component: backup-cleanup
        spec:
          restartPolicy: OnFailure
          serviceAccountName: backup-service-account
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
            runAsGroup: 1001
            fsGroup: 1001
          containers:
          - name: backup-cleanup
            image: alpine:3.18
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - |
              set -e
              
              echo "Starting backup cleanup process..."
              
              # Install aws-cli for S3 operations
              apk add --no-cache aws-cli curl
              
              RETENTION_DAYS=${BACKUP_RETENTION_DAYS:-7}
              echo "Cleaning up local backups older than $RETENTION_DAYS days..."
              
              # Clean local backups
              find /backups -name "*.dump" -type f -mtime +$RETENTION_DAYS -delete 2>/dev/null || true
              find /backups -name "*.sha256" -type f -mtime +$RETENTION_DAYS -delete 2>/dev/null || true
              
              # List remaining files
              echo "Remaining backup files:"
              ls -la /backups/ || echo "No backup files found"
              
              # Clean S3 backups (if configured)
              if [ -n "${AWS_S3_BACKUP_BUCKET:-}" ]; then
                echo "S3 backup cleanup handled by lifecycle policies"
              fi
              
              echo "Backup cleanup completed successfully"
            env:
            - name: BACKUP_RETENTION_DAYS
              valueFrom:
                configMapRef:
                  name: backup-config
                  key: retention-days
                  optional: true
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretRef:
                  name: backup-credentials
                  key: aws-access-key-id
                  optional: true
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretRef:
                  name: backup-credentials
                  key: aws-secret-access-key
                  optional: true
            - name: AWS_S3_BACKUP_BUCKET
              valueFrom:
                configMapRef:
                  name: backup-config
                  key: s3-bucket
                  optional: true
            resources:
              requests:
                memory: "64Mi"
                cpu: "50m"
              limits:
                memory: "128Mi"
                cpu: "100m"
            volumeMounts:
            - name: backup-storage
              mountPath: /backups
            securityContext:
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              runAsNonRoot: true
              runAsUser: 1001
              capabilities:
                drop:
                - ALL
          volumes:
          - name: backup-storage
            persistentVolumeClaim:
              claimName: backup-storage-pvc

---
# Service Account for backup operations
apiVersion: v1
kind: ServiceAccount
metadata:
  name: backup-service-account
  namespace: production
  labels:
    app: ads-pro-platform
    component: backup

---
# ConfigMap for backup configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: backup-config
  namespace: production
  labels:
    app: ads-pro-platform
    component: backup
data:
  retention-days: "7"
  s3-bucket: "ads-pro-platform-backups"
  backup-schedule-full: "0 2 * * 0"
  backup-schedule-incremental: "0 */6 * * *"

---
# Persistent Volume Claim for backup storage
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: backup-storage-pvc
  namespace: production
  labels:
    app: ads-pro-platform
    component: backup
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
  storageClassName: gp2